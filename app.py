import os

import streamlit as st

# Add IS_STREAMLIT to the environment
os.environ["IS_STREAMLIT"] = "True"
from main import answer  # noqa: E402

correct_password = st.secrets["general"]["password"]

if "password_valid" not in st.session_state:
    st.session_state.password_valid = False

# Password protection
if not st.session_state.password_valid:
    password = st.text_input("Password", type="password")
    if password == correct_password:
        st.session_state.password_valid = True
        st.success("Correct password")
        # Reload the app to show the main interface
        st.rerun()
    elif password == "":
        st.stop()
    else:
        st.error("Incorrect password")
        st.stop()
else:
    col1, _, col2 = st.columns([2, 4, 2])
    with col1:
        st.image(
            "https://www.drias-climat.fr/public/images/logo_final+Etat.jpg", width=300
        )
    with col2:
        st.image("https://www.drias-eau.fr/public/images/Logo_DRIAS.jpg", width=125)

    st.title("üí¨‚òÄÔ∏è Chatbot DRIAS")

    st.write(
        "Bienvenue sur le chatbot DRIAS, un assistant virtuel qui vous aidera √† trouver des informations en se basant\
         sur les donn√©es du site [DRIAS](https://www.drias-climat.fr/).\n\nLorsque l'option *use rag* est activ√©e, le\
         chatbot va parcourir l'ensemble des textes pr√©sents sur le site et identifie un nombre *Number of retrieved\
         chunks* de paragraphes qui ont l'air pertinents pour r√©pondre √† la question. Puis une instruction sera donn√©e\
         au *generative model*: \"voici des documents : [paragraphe 1], [paragraphe 2], etc. A partir de ces documents\
        , r√©pond √† la question : [question utilisateur]\".\n\nPour commencer, s√©lectionnez les param√®tres de votre\
         choix dans la barre lat√©rale puis posez votre question dans la zone de texte ci-dessous."
    )

    # Sidebar
    st.sidebar.title("Parameters")

    n_samples = st.sidebar.slider(
        "Number of retrieved chunks :",
        min_value=5,
        max_value=100,
        value=40,
        help="Nombre de morceaux de documents provenant du site DRIAS r√©cup√©r√©s pour chaque question.\nPlus le nombre\
             est grand, plus le chatbot aura de contexte mais plus le temps de calcul sera long.\nLorsque le nombre de\
             morceaux est √©lev√©, il est recommand√© d'utiliser un mod√®le de reranking.",
    )
    use_rag = st.sidebar.checkbox(
        "Use rag",
        value=True,
        help="Utiliser le RAG (Retrieval augmented generation) permet de g√©n√©rer des r√©ponses plus pr√©cises en se\
             basant sur des morceaux de documents r√©cup√©r√©s.\nSi cette option est d√©sactiv√©e, le chatbot g√©n√©rera des\
             r√©ponses sans se baser sur le site DRIAS.",
    )
    generative_model = st.sidebar.selectbox(
        "Choose a generative model:",
        ["Llama-3.2-3B-Instruct", "Chocolatine-3B-Instruct-DPO-v1.0"],
        help="Mod√®le de g√©n√©ration de texte utilis√© pour r√©pondre aux questions. \nLLama-3.2-3B-Instruct\
             est recommand√©.",
    )
    reranker_model = st.sidebar.selectbox(
        "Choose a reranker model:",
        ["bge-reranker-v2-m3", "No reranker"],
        help="Mod√®le de reranking utilis√© selctionner les morceaux de documents les plus important parmis ceux\
             recup√©r√©s et les classer par ordre de pertinence. \nUtiliser un mod√®le de reranking permet d'am√©liorer\
             la qualit√© des r√©ponses mais augmente le temps de calcul.\nIl est recommand√© d'utiliser un mod√®le de\
             reranking lorsque le nombre de morceaux de documents r√©cup√©r√©s est √©lev√©.",
    )

    if reranker_model == "No reranker":
        reranker_model = ""

    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Accept user input
    if prompt := st.chat_input("What is up?"):
        # Display user message in chat message container
        with st.chat_message("user"):
            st.markdown(prompt)
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})

        # Streamed response emulator
        def response_generator():
            response = answer(
                prompt,
                generative_model=generative_model,
                n_samples=n_samples,
                use_rag=use_rag,
                reranker=reranker_model,
            )

            yield response

        # Display assistant response in chat message container
        with st.chat_message("assistant"):
            response = st.write_stream(response_generator())

        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": response})
